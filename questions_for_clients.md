# Questions for Thursday Meeting - YWCA Data Platform

## üéØ KEY INSIGHT FROM THE SURVEY

**You've now seen the 61-page survey.** This is critical context:
- Survey takes chapters 3 weeks to complete
- Requires digging through files (990s, budgets, strategic plans)
- Data becomes "stale" after submission
- Chapters can't access their own data afterward
- No way to see other chapters' data
- Headquarters has to survey repeatedly to get updates

**THE PLATFORM'S CORE PURPOSE: Transform this 61-page survey from a once-a-year marathon into continuous, easy data tracking that chapters can use for their own benefit.**

---

## ‚≠ê THE 5 MOST IMPORTANT QUESTIONS TO ASK

**Ask these first - they'll shape everything else:**

### 1. Survey Replacement Strategy
**"Is the platform meant to completely replace this annual survey, or transform it into continuous data entry?"**
- If replace: Do some questions still need to be asked annually?
- If transform: Which survey questions become ongoing data entry vs. one-time setup?
- What happens to the survey workflow after the platform launches?

### 2. Data Import from Existing Survey
**"You have October 2024 survey data. Should we import this as baseline data for chapters?"**
- Do chapters need to see their historical survey responses?
- Is this data editable or locked as historical reference?
- Does this become the "starting point" for their platform data?

### 3. Update Frequency by Data Type
**"How often should chapters update different types of data?"**
- **Organization info** (address, staff count, locations): Annually? As changes occur?
- **Demographics served**: Monthly? Quarterly? Annually?
- **Financial data**: After fiscal year? Monthly? When 990 filed?
- **Programs**: Added immediately when launched? Updated how often?
- **Impact evaluations**: Continuous assessment? Quarterly? Annually?
- **Service numbers** (people served): Real-time? Monthly? Annually?

### 4. MVP Scope Definition
**"What MUST be in version 1 for this to be useful to chapters?"**
- Data warehouse only, or must include trauma graph?
- Which entities: Organizations + Programs only? Or all entities?
- Which reports: Grant reports only? Or all report types?
- Can we launch with basic features and add advanced analytics later?
- What's the absolute minimum for YWCA to say "yes, this is useful"?

### 5. Cross-Chapter Data Visibility
**"What can chapters see about other chapters' data?"**
- Can Buffalo see Rochester's program names? Descriptions? Impact data?
- Only aggregated/anonymized data? ("20 chapters in NY do childcare programs")
- Can they see individual chapter identities or just aggregate numbers?
- What's public within the network vs. private to each chapter?
- Does this change if chapters opt-in to sharing?

---

## üî¨ NEW: Research Infrastructure Questions

**Based on the "About the Impact Survey" document, this is research infrastructure, not just data storage:**

### 6. Research vs. Operations Balance
**"I see this is research infrastructure with an impact measurement framework. What's the balance between research needs and operational needs?"**
- Is the primary purpose to support 10Seven's research and publications?
- Or is it primarily to help chapters with operations (grant writing)?
- How do we balance these potentially competing needs?
- Should chapters even see the impact framework (pillars, violence types) or should we abstract it?

### 7. Impact Framework Enforcement
**"How rigidly should the platform enforce the impact framework categories?"**
- Must all programs fit into: Racial Violence, Gender-Based Violence, or Economic Violence?
- What if a chapter's program doesn't fit the framework?
- Can chapters add their own categories or must they use your controlled vocabulary?
- The document says impact areas "may need more specificity as we learn" - how flexible should the system be?

### 8. Statistical Analysis Capabilities
**"You plan to do correlation analysis across many variables. What statistical capabilities need to be in the platform?"**
- Basic only (percentages, averages, year-over-year)?
- Advanced (correlations, regressions, multivariate analysis)?
- Or will you export to Qualtrics/SPSS/R for analysis?
- Do you need cross-tabulations? Filtering by multiple variables?
- Should the platform generate the correlation tables mentioned in your doc?

### 9. Data Quality for Research
**"What validation is needed to ensure research-quality data?"**
- Which fields are required for research vs. optional for operations?
- Should there be coherence checks? (e.g., evidence type must match claimed impact type)
- Review/approval process before data is used in research publications?
- How do you handle data quality issues from less sophisticated chapters?

### 10. Built-In Definitions and Help
**"The survey toolkit includes a glossary with precise definitions. Should these be built into the platform?"**
- Tooltip/help text on every field with official definitions?
- Searchable glossary within platform?
- Contextual help that explains the framework as users enter data?
- Links to examples of good data entry?

---

## Priority 1: User Workflow Walkthroughs (REQUEST THESE!)

Ask them to walk you through these specific scenarios step-by-step:

### Walkthrough 1: Chapter User Journey
**"Can you walk me through what happens when a chapter director logs in for the first time and wants to enter data about their programs?"**

- What does the dashboard look like when they first log in?
- What options do they see?
- How do they add new program data? (Forms? Bulk upload? Both?)
- What fields are they filling out?
- What happens after they submit?
- What can they see immediately?

### Walkthrough 2: Comparing Data with Trauma Graph
**"Walk me through a real example: A chapter wants to understand financial trauma in their area and compare it to their client data."**

- Where do they click to access the trauma graph?
- How do they select their geographic area?
- What does the split-screen view actually show? (Can you sketch this?)
- What data from their side appears on the left/right?
- What trauma graph data appears on the other side?
- Can they export/save this comparison?
- What actions can they take based on what they see?

### Walkthrough 3: Cross-Chapter Discovery
**"Show me how a chapter in Buffalo would discover what a chapter in Rochester is doing."**

- Where do they navigate to see other chapters?
- What information can they see about other chapters? What can't they see?
- Can they filter/search? By what criteria?
- What happens if they want to "adopt" or learn from another chapter's program?

### Walkthrough 4: Headquarters National View
**"How does the national headquarters use this differently than a local chapter?"**

- What aggregated data do they see?
- Can they drill down into individual chapters?
- How do they generate reports for funders?
- What's the workflow for the surveys they currently send out?

### Walkthrough 5: Grant Writing Use Case
**"Take me through a real scenario: A chapter needs to write a grant application by Friday. How does this platform help them?"**

- What reports do they generate?
- What specific data points do they need?
- What format does the output need to be? (PDF? Excel? Both?)
- Can they customize reports or are they templated?

### Walkthrough 6: Survey Replacement Scenario (NEW!)
**"Walk me through how the platform replaces the annual survey. What's different?"**

OLD WAY (Survey):
- Chapter receives survey link
- Spends 3 weeks filling it out
- Never sees data again

NEW WAY (Platform):
- Chapter does... what exactly?
- How often do they log in?
- What triggers them to update data?
- How does HQ get the data they used to get from surveys?

### Walkthrough 7: Adding a New Program (BASED ON SURVEY Q27-34)
**"A chapter just launched a new domestic violence housing program. Walk me through how they add it to the platform."**

Based on the survey, they need to enter:
- Program name
- Objectives/purpose
- Languages delivered in
- Impact areas (Women & Girls Empowerment, Health & Safety, Racial Justice)
- Type of violence addressed (Racial, Gender-Based, Economic)
- Resources leveraged
- Activities (specific to violence type)
- How it addresses violence
- Evidence of impact

Questions:
- Do they enter all this at once or progressively?
- Can they save a draft and come back?
- How do they update program info as it evolves?

---

## Priority 2: Data Questions

### Data Input (NOW INFORMED BY THE SURVEY!)
1. **Survey transformation: "I've reviewed the 61-page survey. Is the goal to transform this survey into the platform's data model?"**
   - Should every survey question become a field in the platform?
   - Or are some questions only for annual surveys, not continuous tracking?

2. **Data migration from October 2024 survey:**
   - Should we import existing survey responses as baseline data?
   - Do chapters need to see their historical survey answers?
   - Should this data be editable or locked as historical?

3. **Update frequency for different data types:**
   - **Demographics served**: Updated how often? (monthly? quarterly? as changes occur?)
   - **Financial data**: Updated when? (monthly, after fiscal year close, when 990 filed?)
   - **Programs**: Added immediately when launched? Updated how often?
   - **Impact evaluations**: Continuous or periodic assessment?
   - **Staffing numbers**: Real-time or periodic?

4. **Conditional logic from survey:**
   - The survey has lots of "Display if..." logic (e.g., file upload if selected)
   - Should the platform replicate this conditional logic?
   - Or is the platform more flexible since data entry isn't one-time?

5. **File management:**
   - Survey allows uploading 990s, budgets, strategic plans, impact docs
   - Should platform have version control? (e.g., upload 2023 990, then 2024 990)
   - Should files be tagged/categorized?
   - Who can access uploaded files? (just the chapter, or network-wide?)

### Data Output
5. **What specific reports do they need?**
   - Can you show me examples of reports they wish they had?
   - What does a "compelling grant narrative" report look like?
   - What formats? (PDF, Excel, Word, presentations?)

6. **What automatic calculations are most important?**
   - You mentioned percentages - what else?
   - Averages? Trends over time? Comparisons?
   - Geographic aggregations?

### Trauma Graph Data
7. **What exactly is in the trauma graph dataset?**
   - What metrics/dimensions? (financial trauma, domestic violence severity - what else?)
   - What geographic levels? (state, county, city, neighborhood, zip code?)
   - How often is this data updated?
   - Where does this data come from? (10Seven's research - but what's the source?)

8. **How does trauma graph data map to chapter data?**
   - By geography only?
   - By demographic characteristics?
   - By program type?

---

## Priority 3: Technical Architecture Questions

### Multi-Tenancy
9. **What data can chapters see across the system?**
   - Can Buffalo see Rochester's raw client data? (I assume NO)
   - Can they see aggregated/anonymized data?
   - What's public within the network vs. private to each chapter?

10. **What can national headquarters see vs. chapters?**
    - Full access to all chapter data?
    - Separate permission levels?
    - Can they edit chapter data or only view?

### Licensing & Access
11. **How does licensing affect features?**
    - If a chapter only licenses the data warehouse (not trauma graph), what features are hidden?
    - If someone only licenses trauma graph (like Urban Institute), what do they see?
    - Do individual chapters control their own license or does national HQ?

12. **User accounts: How does authentication work?**
    - Individual users or shared chapter accounts?
    - How many users per chapter?
    - Role types? (admin, viewer, data entry, etc.)

### Integration with Existing Tools
13. **Qualtrics integration: What's the priority here?**
    - Do you want to keep using Qualtrics for surveys?
    - Should survey data automatically flow into this platform?
    - API integration or manual export/import?

14. **What about Max QDA, Tableau, Atlas?**
    - Should this platform replace them entirely?
    - Or integrate with them?
    - What features from those tools do you want to replicate?

---

## Priority 4: Scope & Phasing Questions

### MVP Definition
15. **What's the minimum viable product for YWCA launch?**
    - What must be in version 1?
    - What can wait for version 2?
    - What's the timeline expectation?

16. **Which component is more urgent?**
    - Data warehouse or trauma graph?
    - Can we build/launch them separately?

### Feature Prioritization
17. **If you had to rank these features by importance, what's the order?**
    - Data entry/storage
    - Automatic calculations/synthesis
    - Report generation
    - Cross-chapter discovery
    - Trauma graph visualization
    - Split-screen comparison
    - Survey tools
    - Mobile access

18. **What's a "must-have" vs. "nice-to-have"?**
    - Advanced statistical analysis (regressions)?
    - Data visualization/charts?
    - Real-time collaboration?
    - Notification/alerts?
    - Mobile app or responsive web is enough?

---

## Priority 5: Success Criteria & Use Cases

### Measuring Success
19. **How will you know this platform is successful?**
    - Adoption rate by chapters?
    - Grants won?
    - Time saved?
    - Data quality improvement?

20. **What would cause this project to fail?**
    - Too complex?
    - Too slow?
    - Missing key features?
    - Poor adoption?

### Real Examples
21. **Can you show me the actual YWCA impact survey?**
    - Walk through each section
    - What questions are asked?
    - What do the responses look like?

22. **Can you show me real (anonymized) data?**
    - Sample data from a chapter
    - What trauma graph data looks like
    - Example reports they currently struggle to create

---

## Priority 6: Process & Workflow Questions

### Data Governance
23. **Who owns what data?**
    - Chapters own their data - what does that mean technically?
    - Can they delete their data?
    - Can they export everything and leave?
    - What happens if a chapter closes?

24. **Data quality: Who ensures accuracy?**
    - Is there data validation?
    - Who reviews/approves data entries?
    - What happens if national HQ sees bad data from a chapter?

### Support & Maintenance
25. **Who will support end users?**
    - 10Seven team?
    - National YWCA staff?
    - Each chapter self-supports?

26. **How tech-savvy are the actual users?**
    - Can you describe the least technical person who will use this?
    - What's their comfort level with technology?
    - What devices will they use? (Desktop, tablet, phone?)

---

## Key Artifacts to Request

### Please share these before or during the meeting:

- ‚úÖ YWCA impact survey (full version)
- ‚úÖ Previous technical proposal
- ‚úÖ Sample data from chapters (anonymized)
- ‚úÖ Example reports they want to generate
- ‚úÖ Screenshots/demos of Max QDA, Tableau, Atlas showing what features they use
- ‚úÖ Current trauma graph mockups you've created
- ‚úÖ Sketches/wireframes of what they envision (if any exist)
- ‚úÖ List of all data fields/metrics they want to track

---

## Suggested Meeting Flow

**Part 1 (30 min): Understand their current tools**
- Show me how you use each platform today
- What do you love about each?
- What frustrates you?

**Part 2 (45 min): User journey walkthroughs**
- Walk through the 5 scenarios above
- Draw/sketch the screens as we talk
- Identify happy paths and edge cases

**Part 3 (30 min): Data deep dive**
- Show me the survey and real data
- Explain trauma graph in detail
- Map data inputs to outputs

**Part 4 (15 min): Priorities and next steps**
- What MUST be in v1?
- What's the timeline?
- What materials do I still need?

---

## Questions You Should Be Able to Answer After Thursday

By the end of the meeting, you should clearly understand:

1. ‚úÖ **Who are the users** and what do they need to do?
2. ‚úÖ **What data goes in** and what data comes out?
3. ‚úÖ **How do the two components** (warehouse + trauma graph) work together?
4. ‚úÖ **What does success look like** for a chapter using this?
5. ‚úÖ **What's the MVP** vs. future enhancements?
6. ‚úÖ **What are the technical constraints** (performance, security, compliance)?
7. ‚úÖ **What's the timeline** and budget considerations?

---

## Red Flags to Watch For

Listen for these potential issues:

‚ö†Ô∏è **Vague requirements**: If they can't describe specific workflows, ask for real examples
‚ö†Ô∏è **Scope creep**: If everything is "must have," push back and prioritize
‚ö†Ô∏è **Unrealistic expectations**: If they expect this built in 2 weeks, manage expectations
‚ö†Ô∏è **Missing stakeholders**: If YWCA users aren't involved, that's a risk
‚ö†Ô∏è **Unclear data**: If they can't show you actual data examples, that's a blocker

---

## Your Goal for Thursday

**Walk away with enough clarity to create:**
1. A technical architecture proposal
2. Feature prioritization (MVP vs. v2)
3. Realistic timeline estimate
4. List of any remaining unknowns

**You should be able to answer: "What am I actually building and for whom?"**

---

## Pro Tips

- **Draw everything**: Sketch screens, data flows, architecture diagrams during the call
- **Use their language back**: When they say "data intranet," use that term in your proposals
- **Show, don't tell**: Ask them to share their screen and show you their current tools
- **Real examples only**: Every time they describe a feature, ask "can you show me a real example?"
- **Repeat back**: After each section, summarize what you heard and confirm understanding
- **Take notes on pain points**: When they say "this frustrates us," write it down - that's a key requirement

Good luck with your Thursday meeting!
